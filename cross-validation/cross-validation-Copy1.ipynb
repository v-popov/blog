{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short Intro:\n",
    "I assume you know the basic idea [https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation] behind Cross-Validation (CV). Here are just few points that I consider to be important.\n",
    "\n",
    "1. CV is widely used for model selection because it allows you to estimate the performance of the fitted model on an unseen data.\n",
    "    \n",
    "\n",
    "2. Typically you want to use:\n",
    "    - KFold CV [https://scikit-learn.org/stable/modules/cross_validation.html#k-fold] for regression problems \n",
    "    - StratifiedKFold CV [https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold] for classification problems (especially if the distribution of target labels is not uniform) \n",
    "\n",
    "In this article we will stick to one model (LGBMRegressor [https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html]) and use cross-validation to select its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (506, 13)\n",
      "y Shape: (506,)\n"
     ]
    }
   ],
   "source": [
    "print('X Shape: {}\\ny Shape: {}'.format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search [https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search] performs an exhaustive search over the specified range of hyperparameters (grid). For this method you need to specify every single value for each parameter (which can be tricky, especially for the continious value parameters) that you want your model to try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "              'max_depth': np.arange(2, 7),\n",
    "              'learning_rate': np.arange(0.05, 0.51, 0.05),\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = GridSearchCV(LGBMRegressor(random_state=1), \n",
    "                  param_grid, \n",
    "                  cv=kfold, \n",
    "                  scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "                                     colsample_bytree=1.0,\n",
       "                                     importance_type='split', learning_rate=0.1,\n",
       "                                     max_depth=-1, min_child_samples=20,\n",
       "                                     min_child_weight=0.001, min_split_gain=0.0,\n",
       "                                     n_estimators=100, n_jobs=-1, num_leaves=31,\n",
       "                                     objective=Non...dom_state=1,\n",
       "                                     reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "                                     subsample=1.0, subsample_for_bin=200000,\n",
       "                                     subsample_freq=0),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'learning_rate': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ]),\n",
       "                         'max_depth': array([2, 3, 4, 5, 6])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.25, 'max_depth': 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.25, max_depth=3,\n",
       "              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "              random_state=1, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.931569841987132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.037414969190734"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_train, cv.best_estimator_.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.347274902683115"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_test, cv.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major downsides of grid search CV is that it can the case, that for example learning_rate=0.45 always leads to terrible performance no matter what values other parameters have, but in the example above grid search CV the value learning_rate=0.45 is still used 5 times which leads to basically wasting of these 5 trials. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {'max_depth': randint(low=2, high=7),\n",
    "                       'learning_rate' : uniform(loc=0.05, scale=0.5-0.05)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RandomizedSearchCV(LGBMRegressor(random_state=1), \n",
    "                        param_distributions, \n",
    "                        n_iter=45,\n",
    "                        cv=kfold, \n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=5, random_state=1, shuffle=True),\n",
       "                   error_score='raise-deprecating',\n",
       "                   estimator=LGBMRegressor(boosting_type='gbdt',\n",
       "                                           class_weight=None,\n",
       "                                           colsample_bytree=1.0,\n",
       "                                           importance_type='split',\n",
       "                                           learning_rate=0.1, max_depth=-1,\n",
       "                                           min_child_samples=20,\n",
       "                                           min_child_weight=0.001,\n",
       "                                           min_split_gain=0.0, n_estimators=100,\n",
       "                                           n_jobs=-1, num_leaves=31,\n",
       "                                           objecti...\n",
       "                                           subsample_freq=0),\n",
       "                   iid='warn', n_iter=45, n_jobs=None,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001B028E6E940>,\n",
       "                                        'max_depth': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001B028E6EBA8>},\n",
       "                   pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "                   return_train_score=False, scoring='neg_mean_squared_error',\n",
       "                   verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2861597195917005, 'max_depth': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.2861597195917005,\n",
       "              max_depth=3, min_child_samples=20, min_child_weight=0.001,\n",
       "              min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n",
       "              objective=None, random_state=1, reg_alpha=0.0, reg_lambda=0.0,\n",
       "              silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "              subsample_freq=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.869192784679505"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.836994185753246"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_train, cv.best_estimator_.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.382017864091782"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_test, cv.best_estimator_.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, hp, fmin, space_eval, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTraining:\n",
    "\n",
    "    def __init__(self, X, y, params_space, n_trials, cv_scoring_metric, cv):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y \n",
    "        self.params_space = params_space\n",
    "        self.n_trials = n_trials\n",
    "        self.cv_scoring_metric = cv_scoring_metric\n",
    "        self.cv = cv\n",
    "        self.trials = Trials()\n",
    "\n",
    "    def _objective(self, params):\n",
    "        estimator = LGBMRegressor(**params, random_state=1)\n",
    "        score = cross_val_score(estimator, self.X, self.y, \n",
    "                                scoring=self.cv_scoring_metric, cv=self.cv).mean()\n",
    "        return -score\n",
    "        \n",
    "    def optimize(self):\n",
    "        return fmin(self._objective,\n",
    "                    self.params_space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=self.n_trials,\n",
    "                    trials=self.trials,\n",
    "                    rstate=np.random.RandomState(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_space = {'learning_rate' : hp.uniform('learning_rate', 0.05, 0.5),\n",
    "                'max_depth' : hp.choice('max_depth', np.arange(2,7))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training = ModelTraining(X=X_train, \n",
    "                               y=y_train, \n",
    "                               params_space=params_space,\n",
    "                               n_trials=50, \n",
    "                               cv_scoring_metric='neg_mean_squared_error', \n",
    "                               cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████| 50/50 [00:08<00:00,  5.97it/s, best loss: 11.870490843729723]\n"
     ]
    }
   ],
   "source": [
    "best_params = model_training.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2556072866275003, 'max_depth': 1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params\n",
    "# keep in mind, that when 'hp.choice' is used, the return of the fmin function contains \n",
    "# the index of the parameter value provided in the corresponding hp.choice range of values\n",
    "# thus, 'max_depth': 3 means that \"third parameter in the np.arange(2,7) was picked as optimal\"\n",
    "# as np.arange(2,7) -> [2,3,4,5,6], the third parameter has the value of 5 (indexing starts with 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2556072866275003, 'max_depth': 3}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can retrieve the values of the selected parameters by using space_eval:\n",
    "space_eval(params_space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.2556072866275003,\n",
       "              max_depth=3, min_child_samples=20, min_child_weight=0.001,\n",
       "              min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n",
       "              objective=None, random_state=1, reg_alpha=0.0, reg_lambda=0.0,\n",
       "              silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "              subsample_freq=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = LGBMRegressor(**space_eval(params_space, best_params), random_state=1)\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0059894557008233"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_train, best_model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.54208304337233"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_test, best_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "optuna.logging.set_verbosity('WARNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import create_study, samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTraining:\n",
    "\n",
    "    def __init__(self, X, y, n_trials, cv_scoring_metric, cv,\n",
    "                 sampler_seed=1):\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_trials = n_trials\n",
    "        self._cv_scoring_metric = cv_scoring_metric\n",
    "        self._cv = cv\n",
    "        self.study = None\n",
    "        self._sampler_seed= sampler_seed\n",
    "\n",
    "    def _objective(self, trial):\n",
    "        params = {\n",
    "            'learning_rate': trial.suggest_uniform('learning_rate', 0.05, 0.5),\n",
    "            'max_depth': trial.suggest_int('max_depth', 2, 7-1),\n",
    "        }\n",
    "            \n",
    "        model = LGBMRegressor(**params, random_state=1)\n",
    "        score = cross_val_score(model, \n",
    "                                self.X, \n",
    "                                self.y, \n",
    "                                scoring=self._cv_scoring_metric,\n",
    "                                cv=self._cv).mean()\n",
    "        return score       \n",
    "\n",
    "    def optimize(self):\n",
    "        self.study = create_study(sampler=samplers.TPESampler(seed=self._sampler_seed),\n",
    "                                  direction='maximize')\n",
    "        self.study.optimize(self._objective, n_trials=self.n_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training = ModelTraining(X=X_train, \n",
    "                               y=y_train, \n",
    "                               n_trials=50, \n",
    "                               cv_scoring_metric='neg_mean_squared_error', \n",
    "                               cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.24545070025296128, 'max_depth': 3}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_training.study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.623597255551626"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_training.study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "              importance_type='split', learning_rate=0.24545070025296128,\n",
       "              max_depth=3, min_child_samples=20, min_child_weight=0.001,\n",
       "              min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n",
       "              objective=None, random_state=1, reg_alpha=0.0, reg_lambda=0.0,\n",
       "              silent=True, subsample=1.0, subsample_for_bin=200000,\n",
       "              subsample_freq=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = LGBMRegressor(**model_training.study.best_params, random_state=1)\n",
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.22107239312464"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_train, best_model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.41184036504707"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y_test, best_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links\n",
    "\n",
    "- Predefined sklearn evaluation metrics: https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules\n",
    "\n",
    "\n",
    "- Scipy distributions: https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "\n",
    "\n",
    "- Hyperopt parameter expressions: https://github.com/hyperopt/hyperopt/wiki/FMin#21-parameter-expressions\n",
    "\n",
    "\n",
    "- Optuna suggest options: https://optuna.readthedocs.io/en/latest/reference/trial.html#optuna.trial.Trial.suggest_categorical"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
